{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import transformers\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from math import sqrt\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = pd.read_csv(r\"C:\\Users\\HP\\Desktop\\scaai\\Bearing 1_1 time_domain.csv\")\n",
    "frequency = pd.read_csv(r\"C:\\Users\\HP\\Desktop\\scaai\\Bearing 1_1 frequency_domain.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.drop(\"Unnamed: 0\", axis = 1, inplace = True)\n",
    "time.drop(\"Signal\", axis = 1, inplace = True)\n",
    "frequency.drop(\"Unnamed: 0\", axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = time[\"Time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([time.drop(\"Time\", axis = 1),frequency.drop(\"Time\", axis = 1)], axis = 1)\n",
    "X = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features: Index(['crest_factor_Y', 'variance_X', 'standard_deviations_Y', 'p2p_X',\n",
      "       'p2p_Y', 'Entropy_X', 'Entropy_Y', 'Skewness_X', 'wave_factor_X',\n",
      "       'peak_factor_Y', 'skewness_coefficent_X', 'freq_mean_fx',\n",
      "       'freq_peak_fx', 'spectral_entropy_fx', 'spectral_skew_fx', 'psd_pg_fx',\n",
      "       'spectral_entropy_fy', 'spectral_flatness_fy', 'ZCR_fy', 'psd_pg_fy'],\n",
      "      dtype='object')\n",
      "Accuracy on Test Set: 0.96930828736409\n"
     ]
    }
   ],
   "source": [
    "'''''\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "clf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "sfs = SequentialFeatureSelector(clf, n_features_to_select=20, direction='forward', scoring='r2', cv=2)\n",
    "\n",
    "sfs.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "selected_features = X.columns[sfs.get_support()]\n",
    "\n",
    "X_train_selected = sfs.transform(X_train)\n",
    "X_test_selected = sfs.transform(X_test)\n",
    "\n",
    "clf.fit(X_train_selected, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test_selected)\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Selected Features: {selected_features}\")\n",
    "print(f\"Accuracy on Test Set: {r2}\")\n",
    "'''''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['crest_factor_Y', 'variance_X', 'standard_deviations_Y', 'p2p_X',\n",
    "       'p2p_Y', 'Entropy_X', 'Entropy_Y', 'Skewness_X', 'wave_factor_X',\n",
    "       'peak_factor_Y', 'skewness_coefficent_X',   'freq_mean_fx',\n",
    "       'freq_peak_fx', 'spectral_entropy_fx', 'spectral_skew_fx', 'psd_pg_fx',\n",
    "       'spectral_entropy_fy', 'spectral_flatness_fy', 'ZCR_fy', 'psd_pg_fy']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Validation Loss: 1.3324\n",
      "Epoch 2/1000, Validation Loss: 0.9225\n",
      "Epoch 3/1000, Validation Loss: 0.9395\n",
      "Epoch 4/1000, Validation Loss: 0.8615\n",
      "Epoch 5/1000, Validation Loss: 1.0078\n",
      "Epoch 6/1000, Validation Loss: 1.0203\n",
      "Epoch 7/1000, Validation Loss: 1.1656\n",
      "Epoch 00008: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 8/1000, Validation Loss: 1.0059\n",
      "Epoch 9/1000, Validation Loss: 0.9561\n",
      "Epoch 10/1000, Validation Loss: 1.0635\n",
      "Epoch 11/1000, Validation Loss: 0.9978\n",
      "Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 12/1000, Validation Loss: 0.8728\n",
      "Epoch 13/1000, Validation Loss: 0.9232\n",
      "Early stopping!\n",
      "RMSE for crest_factor_Y: 1.5658\n",
      "R2 Score for crest_factor_Y: -0.2020\n",
      "Epoch 1/1000, Validation Loss: 1744.3049\n",
      "Epoch 2/1000, Validation Loss: 1710.4954\n",
      "Epoch 3/1000, Validation Loss: 1712.9743\n",
      "Epoch 4/1000, Validation Loss: 1713.7687\n",
      "Epoch 5/1000, Validation Loss: 1712.3329\n",
      "Epoch 00006: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 6/1000, Validation Loss: 1726.0133\n",
      "Epoch 7/1000, Validation Loss: 1714.7643\n",
      "Epoch 8/1000, Validation Loss: 1716.9665\n",
      "Epoch 9/1000, Validation Loss: 1715.2341\n",
      "Epoch 00010: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 10/1000, Validation Loss: 1719.2721\n",
      "Epoch 11/1000, Validation Loss: 1719.9224\n",
      "Early stopping!\n",
      "RMSE for variance_X: 4.6245\n",
      "R2 Score for variance_X: -0.2140\n",
      "Epoch 1/1000, Validation Loss: 45.5620\n",
      "Epoch 2/1000, Validation Loss: 42.4685\n",
      "Epoch 3/1000, Validation Loss: 44.5858\n",
      "Epoch 4/1000, Validation Loss: 44.1126\n",
      "Epoch 5/1000, Validation Loss: 43.7040\n",
      "Epoch 00006: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 6/1000, Validation Loss: 42.7450\n",
      "Epoch 7/1000, Validation Loss: 37.7209\n",
      "Epoch 8/1000, Validation Loss: 41.1440\n",
      "Epoch 9/1000, Validation Loss: 44.7235\n",
      "Epoch 10/1000, Validation Loss: 43.8803\n",
      "Epoch 00011: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 11/1000, Validation Loss: 42.1808\n",
      "Epoch 12/1000, Validation Loss: 40.0771\n",
      "Epoch 13/1000, Validation Loss: 39.6778\n",
      "Epoch 14/1000, Validation Loss: 39.8334\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.2500e-04.\n",
      "Epoch 15/1000, Validation Loss: 38.7094\n",
      "Epoch 16/1000, Validation Loss: 39.3252\n",
      "Early stopping!\n",
      "RMSE for standard_deviations_Y: 0.5413\n",
      "R2 Score for standard_deviations_Y: 0.0000\n",
      "Epoch 1/1000, Validation Loss: 131.3149\n",
      "Epoch 2/1000, Validation Loss: 133.8307\n",
      "Epoch 3/1000, Validation Loss: 135.4401\n",
      "Epoch 4/1000, Validation Loss: 116.6409\n",
      "Epoch 5/1000, Validation Loss: 136.1766\n",
      "Epoch 6/1000, Validation Loss: 134.1110\n",
      "Epoch 7/1000, Validation Loss: 138.1233\n",
      "Epoch 00008: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 8/1000, Validation Loss: 135.0409\n",
      "Epoch 9/1000, Validation Loss: 132.8300\n",
      "Epoch 10/1000, Validation Loss: 132.6085\n",
      "Epoch 11/1000, Validation Loss: 133.6531\n",
      "Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 12/1000, Validation Loss: 130.7008\n",
      "Epoch 13/1000, Validation Loss: 133.2326\n",
      "Early stopping!\n",
      "RMSE for p2p_X: 15.0841\n",
      "R2 Score for p2p_X: -0.7699\n",
      "Epoch 1/1000, Validation Loss: 26.4081\n",
      "Epoch 2/1000, Validation Loss: 26.8242\n",
      "Epoch 3/1000, Validation Loss: 28.7092\n",
      "Epoch 4/1000, Validation Loss: 27.8407\n",
      "Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 5/1000, Validation Loss: 27.2799\n",
      "Epoch 6/1000, Validation Loss: 27.2422\n",
      "Epoch 7/1000, Validation Loss: 27.5063\n",
      "Epoch 8/1000, Validation Loss: 26.9446\n",
      "Epoch 00009: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 9/1000, Validation Loss: 27.6020\n",
      "Epoch 10/1000, Validation Loss: 27.6675\n",
      "Early stopping!\n",
      "RMSE for p2p_Y: 9.2294\n",
      "R2 Score for p2p_Y: -0.0920\n",
      "Epoch 1/1000, Validation Loss: 11.8987\n",
      "Epoch 2/1000, Validation Loss: 8.4208\n",
      "Epoch 3/1000, Validation Loss: 11.5769\n",
      "Epoch 4/1000, Validation Loss: 11.5059\n",
      "Epoch 5/1000, Validation Loss: 11.1069\n",
      "Epoch 00006: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 6/1000, Validation Loss: 11.5716\n",
      "Epoch 7/1000, Validation Loss: 11.2851\n",
      "Epoch 8/1000, Validation Loss: 11.7032\n",
      "Epoch 9/1000, Validation Loss: 12.6216\n",
      "Epoch 00010: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 10/1000, Validation Loss: 12.6925\n",
      "Epoch 11/1000, Validation Loss: 12.5007\n",
      "Early stopping!\n",
      "RMSE for Entropy_X: 0.7338\n",
      "R2 Score for Entropy_X: -27.3948\n",
      "Epoch 1/1000, Validation Loss: 7.3908\n",
      "Epoch 2/1000, Validation Loss: 6.5738\n",
      "Epoch 3/1000, Validation Loss: 8.4309\n",
      "Epoch 4/1000, Validation Loss: 8.3716\n",
      "Epoch 5/1000, Validation Loss: 8.4116\n",
      "Epoch 00006: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 6/1000, Validation Loss: 7.2450\n",
      "Epoch 7/1000, Validation Loss: 7.7181\n",
      "Epoch 8/1000, Validation Loss: 7.5033\n",
      "Epoch 9/1000, Validation Loss: 7.8867\n",
      "Epoch 00010: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 10/1000, Validation Loss: 8.6371\n",
      "Epoch 11/1000, Validation Loss: 9.0274\n",
      "Early stopping!\n",
      "RMSE for Entropy_Y: 0.3298\n",
      "R2 Score for Entropy_Y: -0.5924\n",
      "Epoch 1/1000, Validation Loss: 10.9122\n",
      "Epoch 2/1000, Validation Loss: 8.9235\n",
      "Epoch 3/1000, Validation Loss: 8.9654\n",
      "Epoch 4/1000, Validation Loss: 8.9205\n",
      "Epoch 5/1000, Validation Loss: 8.9893\n",
      "Epoch 6/1000, Validation Loss: 9.8785\n",
      "Epoch 7/1000, Validation Loss: 10.5616\n",
      "Epoch 00008: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 8/1000, Validation Loss: 9.8820\n",
      "Epoch 9/1000, Validation Loss: 9.8589\n",
      "Epoch 10/1000, Validation Loss: 9.9025\n",
      "Epoch 11/1000, Validation Loss: 9.4729\n",
      "Epoch 00012: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 12/1000, Validation Loss: 9.5564\n",
      "Epoch 13/1000, Validation Loss: 9.7802\n",
      "Early stopping!\n",
      "RMSE for Skewness_X: 0.4581\n",
      "R2 Score for Skewness_X: -0.4911\n",
      "Epoch 1/1000, Validation Loss: 129.6142\n",
      "Epoch 2/1000, Validation Loss: 113.8404\n",
      "Epoch 3/1000, Validation Loss: 129.2157\n",
      "Epoch 4/1000, Validation Loss: 131.7220\n",
      "Epoch 5/1000, Validation Loss: 124.3036\n",
      "Epoch 00006: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 6/1000, Validation Loss: 123.4407\n",
      "Epoch 7/1000, Validation Loss: 123.2382\n",
      "Epoch 8/1000, Validation Loss: 128.3752\n",
      "Epoch 9/1000, Validation Loss: 130.2616\n",
      "Epoch 00010: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 10/1000, Validation Loss: 129.8413\n",
      "Epoch 11/1000, Validation Loss: 129.4747\n",
      "Early stopping!\n",
      "RMSE for wave_factor_X: 1.7833\n",
      "R2 Score for wave_factor_X: -0.9532\n",
      "Epoch 1/1000, Validation Loss: 1.4748\n",
      "Epoch 2/1000, Validation Loss: 1.4743\n",
      "Epoch 3/1000, Validation Loss: 1.7108\n",
      "Epoch 4/1000, Validation Loss: 1.5156\n",
      "Epoch 5/1000, Validation Loss: 1.8320\n",
      "Epoch 00006: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 6/1000, Validation Loss: 1.5626\n",
      "Epoch 7/1000, Validation Loss: 1.7155\n",
      "Epoch 8/1000, Validation Loss: 1.8079\n",
      "Epoch 9/1000, Validation Loss: 1.6307\n",
      "Epoch 00010: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 10/1000, Validation Loss: 1.6732\n",
      "Epoch 11/1000, Validation Loss: 1.6782\n",
      "Early stopping!\n",
      "RMSE for peak_factor_Y: 3.2833\n",
      "R2 Score for peak_factor_Y: -0.1142\n",
      "Epoch 1/1000, Validation Loss: 5.9366\n",
      "Epoch 2/1000, Validation Loss: 6.7748\n",
      "Epoch 3/1000, Validation Loss: 6.3718\n",
      "Epoch 4/1000, Validation Loss: 6.0675\n",
      "Epoch 00005: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 5/1000, Validation Loss: 6.3148\n",
      "Epoch 6/1000, Validation Loss: 6.1110\n",
      "Epoch 7/1000, Validation Loss: 6.3559\n",
      "Epoch 8/1000, Validation Loss: 6.5349\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for col in cols:\n",
    "    X_train_selected = np.array(X_train[col]).reshape(-1,1)\n",
    "    X_test_selected = np.array(X_test[col]).reshape(-1,1)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_standardized = scaler.fit_transform(X_train_selected).tolist()\n",
    "    X_test_standardized = scaler.transform(X_test_selected).tolist()\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "    SEQUENCE_SIZE = 50\n",
    "\n",
    "    def to_sequences(seq_size, obs):\n",
    "        x = []\n",
    "        y = []\n",
    "        for i in range(len(obs) - seq_size):\n",
    "            window = obs[i:(i + seq_size)]\n",
    "            after_window = obs[i + seq_size]\n",
    "            x.append(window)\n",
    "            y.append(after_window)\n",
    "        return torch.tensor(x, dtype=torch.float32).view(-1, seq_size, 1), torch.tensor(y, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    x_train, y_train = to_sequences(SEQUENCE_SIZE, X_train_standardized)\n",
    "    x_test, y_test = to_sequences(SEQUENCE_SIZE, X_test_standardized)\n",
    "\n",
    "    # Setup data loaders for batch\n",
    "    train_dataset = TensorDataset(x_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    test_dataset = TensorDataset(x_test, y_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    class PositionalEncoding(nn.Module):\n",
    "        def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "            super(PositionalEncoding, self).__init__()\n",
    "            self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "            pe = torch.zeros(max_len, d_model)\n",
    "            position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "            div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "            pe[:, 0::2] = torch.sin(position * div_term)\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)\n",
    "            pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "            self.register_buffer('pe', pe)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = x + self.pe[:x.size(0), :]\n",
    "            return self.dropout(x)\n",
    "    class TransformerModel(nn.Module):\n",
    "        def __init__(self, input_dim=1, d_model=256, nhead=4, num_layers=8, dropout=0.1):\n",
    "            super(TransformerModel, self).__init__()\n",
    "\n",
    "            self.encoder = nn.Linear(input_dim, d_model)\n",
    "            self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "            encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=512, dropout=dropout, activation='relu')\n",
    "            self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "            self.decoder = nn.Linear(d_model, 1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.encoder(x)\n",
    "            x = self.pos_encoder(x)\n",
    "            x = self.transformer_encoder(x)\n",
    "            x = self.decoder(x[:, -1, :])\n",
    "            return x\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = TransformerModel().to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "    epochs = 1000\n",
    "    early_stop_count = 0\n",
    "    min_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            x_batch, y_batch = batch\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                x_batch, y_batch = batch\n",
    "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(x_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_losses.append(loss.item())\n",
    "\n",
    "        val_loss = np.mean(val_losses)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_loss < min_val_loss:\n",
    "            min_val_loss = val_loss\n",
    "            early_stop_count = 0\n",
    "        else:\n",
    "            early_stop_count += 1\n",
    "\n",
    "        if early_stop_count >= 10:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {val_loss:.4f}\")\n",
    "    from sklearn.metrics import r2_score\n",
    "\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            x_batch, y_batch = batch\n",
    "            x_batch = x_batch.to(device)\n",
    "            outputs = model(x_batch)\n",
    "            predictions.extend(outputs.squeeze().tolist())\n",
    "\n",
    "    rmse = np.sqrt(np.mean((scaler.inverse_transform(np.array(predictions).reshape(-1, 1)) - scaler.inverse_transform(y_test.numpy().reshape(-1, 1)))**2))\n",
    "    print(f\"RMSE for {col}: {rmse:.4f}\")\n",
    "\n",
    "    r2 = r2_score(scaler.inverse_transform(y_test.numpy().reshape(-1, 1)), scaler.inverse_transform(np.array(predictions).reshape(-1, 1)))\n",
    "    print(f\"R2 Score for {col}: {r2:.4f}\")\n",
    "    scores.append([col, rmse, r2])\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "0.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
